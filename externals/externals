#!/usr/bin/python

import os,sys,re,urllib2,sha,logging,shutil,tempfile
import tarfile,zipfile
import xml.etree.cElementTree 

_logger = logging.getLogger("externals")

class External(object):
    def __init__(self, name, homepage, source, source_sha1):
        self.name = name
        self.homepage = homepage
        self.source = source
        self.source_sha1 = source_sha1

    def filename(self):
        return os.path.basename(self.source)

def do_get(externals):
    for external in externals:
        pass

def _load_externals(dir):
    extfile = "externals.xml"
    exttree = xml.etree.cElementTree.ElementTree()
    exttree.parse(extfile)
    for x in exttree.getiterator('external'):
        source = x.getiterator('source').next()
        yield External(x.attrib['name'], x.attrib['home'],
                       source.attrib['url'], source.attrib['sha1'])

def _external_is_cached(ext, cachedir, error=False):
    ext_file = os.path.join(cachedir, ext.filename())
    if not os.access(ext_file, os.R_OK):
        return False
    dl_sum = sha.new()
    curfile = open(ext_file, 'rb')
    buf = curfile.read(8192)
    while buf:
        dl_sum.update(buf)
        buf = curfile.read(8192)
    curfile.close()
    dl_sum_hex = dl_sum.hexdigest()
    if dl_sum_hex != ext.source_sha1:
        fn = error and _logger.error or _logger.info
        fn("Not using cached file; SHA1 mismatch (downloaded: %s expected: %s)",
           dl_sum_hex, ext.source_sha1)
        return False
    return True

def _uncompr(path, dir):
    if zipfile.is_zipfile(path):
        zf = zipfile.ZipFile(path, 'r')
        # ZipFile sucks (http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/465649)
        pass
    if path.endswith('.tar.gz') or path.endswith('.tar.bz2'):
        tf = tarfile.open(path)
        tf.extractall(dir)
        tf.close()

def main(dir):
    os.chdir(dir)
    urlcachedir = 'urlcache'
    try:
        os.mkdir(urlcachedir)
    except:
        pass
    for ext in _load_externals(dir):
        ext_file = os.path.join(urlcachedir, ext.filename())
        if not _external_is_cached(ext, urlcachedir):
            urlf = urllib2.urlopen(ext.source)
            outf = open(ext_file, 'wb')
            shutil.copyfileobj(urlf, outf)
            urlf.close()
            outf.close()
            if not _external_is_cached(ext, urlcachedir, error=True):
                sys.exit(1)
        if not os.access(ext.name, os.R_OK):
            _logger.info("Uncompressing %s", ext_file)
            tmpd = tempfile.mkdtemp(prefix='ext')
            _uncompr(ext_file, tmpd)
            files = os.listdir(tmpd)
            if len(files) != 1:
                _logger.error("Too many files in %s", tmpd)
                sys.exit(1)
            os.rename(os.path.join(tmpd, files[0]), ext.name)
            _logger.info("Created %s", ext.name)

if __name__ == '__main__' and hasattr(sys.modules['__main__'], '__file__'):
    logging.basicConfig(level=logging.INFO)
    main(os.path.dirname(__file__))

